{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lesson8-Polynomial-Regression.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"hQgPeX9ksV-U","colab_type":"text"},"source":["<img style=\"width:450px;\" src=\"https://durhamcollege.ca/wp-content/uploads/ai-hub-header.jpg\" alt=\"DC Logo\"/>"],"cell_type":"markdown"},{"metadata":{"id":"iKK0tO77sV-X","colab_type":"text"},"source":["# LESSON 8 - Polynomial Regression\n","\n","## <span style=\"color: green\">OVERVIEW</span>\n","\n","<hr />\n","\n",">**Section 1:** <a href=\"#Degrees-of-Polynomials\">Degrees of Polynomials</a>\n","\n",">**Section 2:** <a href=\"#L1-and-L2-Regularization\">L1 and L2 Regularization</a>\n","\n",">**Section 3:** <a href=\"#Expanding-your-Understanding\">Expandng your Understanding</a>\n","\n","<hr />\n","\n","In this lesson we will quickly review Polynomial Regression, also sometimes referred to as a form of Multiple-Linear regression. After which you will do some practical work with linear, logistic and polynomial regression in Lesson 9.\n","\n","In general, polynomial regression is the usage of curved instead of straight lines when analyzing the best fit of a series of data-points. It can be used in multiple different ways but its general purpose is curve fitting.\n","\n","This is achieved using feature selection to determine the data-points that are best represented by the use of a curve as opposed to a line. This is something that you will most likely have to assess visually as you are preparing both your testing and training data.\n","\n","More formally, these curved lines are referred to as an Nth degree polynomial. Polynomial regression allows us to analyze data in multiple ways and find patterns that are potentially abstract. \n","\n","However, this abstract data can occasionally increase overall learning and prediction if used in addition to other methods. Which is really the main usage of polynomial regression in machine learning, as it accounts for flexibility and enables more complex regression when used in conjunction with other methods."],"cell_type":"markdown"},{"metadata":{"id":"u_hP9jUMsV-Z","colab_type":"text"},"source":["<img style=\"width:350px;\" src=\"http://api.ning.com/files/lihD20i2jrw163Eb7AL8EhBuX4o2PanX83Vux57D4mDA2lU5*2p1ER22HLa95iwNbs7hqUwkiHpD5AKJIsYPtdiKIJGLivV0/Capture.PNG\" alt=\"Poly and Linear Example\" />"],"cell_type":"markdown"},{"metadata":{"id":"yBQAlvqasV-b","colab_type":"text"},"source":["### <span style=\"color:#27ae60\">Degrees of Polynomials</span>\n","\n","As you will notice after closely observing the image below, you can determine a polynomial by both the sections between curves; As well as, the function used to define the curve.\n","\n","Understanding what this means in terms of processing data is that you need to consider very carefully which data-points in your data-set have correlations in terms of what you are assessing.\n","\n","You can usually determine this by visualizing data in a graph to determine commonalities, this can also be achieved sometimes by simply analyzing the logical progression of the relationships of the data-headers for each data-point you want to interpolate. (Google Graphviz, it's great for that!)\n"],"cell_type":"markdown"},{"metadata":{"id":"Ke8GmiJusV-c","colab_type":"text"},"source":["<img src=\"Content/nthdegrees.png\" />"],"cell_type":"markdown"},{"metadata":{"id":"fOL6jWSysV-e","colab_type":"text"},"source":["### <span style=\"color:#27ae60\">L1 and L2 Regularization</span>\n","A regression model that uses an L1 regularization technique is called **Lasso Regression** and models which use L2 are called **Ridge Regression**.\n",">*The key difference between these two is the penalty term.*\n","\n","<br />\n","<hr />\n","<br />\n","\n","**Ridge regression** adds a “*squared magnitude*” of the coefficient as a penalty term to the loss function. In the image below the *highlighted* part represents the L2 regularization element.\n","\n","<img src=\"https://cdn-images-1.medium.com/max/1200/1*jgWOhDiGjVp-NCSPa5abmg.png\" style=\"width:450px;\" />\n","<span style=\"padding-left:300px;\">Cost Function</span>\n","<br />\n","\n","Here, if *lambda* is zero then you can imagine we get back OLS (ordinary least squares). However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how *lambda* is chosen. This technique works very well to avoid the over-fitting issue.\n","\n","<br />\n","<hr />\n","<br />\n","\n","**Lasso Regression** (*Least Absolute Shrinkage and Selection Operator*) adds the “*absolute value of magnitude*” of the coefficient as a penalty term to the loss function.\n","\n","<img src=\"https://cdn-images-1.medium.com/max/1200/1*4MlW1d3xszVAGuXiJ1U6Fg.png\" style=\"width:450px;\" />\n","<span style=\"padding-left:300px;\">Cost Function</span>\n","<br />\n","\n","Again, if *lambda* is zero then we will get back OLS (ordinary least squares) whereas very large value will make coefficients zero hence it will under-fit.\n","\n","<br />\n","<hr />\n","<br />\n","\n","The **key difference** between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for **feature selection** in case we have a huge number of features.\n","\n","Traditional methods like cross-validation, stepwise regression to handle overfitting and perform feature selection work well with a small set of features but these techniques are a great alternative when we are dealing with a large set of features.\n","\n","<br />\n","<hr />\n","\n","### <span style=\"color:#27ae60\">Expanding your Understanding</span>\n","\n","1. If\n","2. How\n","3. When\n","4. Where\n","5. Looking at the images above"],"cell_type":"markdown"},{"metadata":{"id":"ZBi0p6PlsV-g","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# Briefly explain your thoughts and try to answer to the best of your ability before seeking references\n","#1\n","\n","#2\n","\n","#3\n","\n","#4\n","\n","#5\n"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"Diq3i225sV-n","colab_type":"text"},"source":["### <span style=\"color:#27ae60\">Practical Application</span>\n","\n","You will find a practical application of polynomial regression at the end of the next lesson (9).\n","\n","Which should give you a better understanding of how it fits into the process of data analysis."],"cell_type":"markdown"},{"metadata":{"id":"19S5XXaTsV-p","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":[""],"cell_type":"code","execution_count":0,"outputs":[]}]}