{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:450px;\" src=\"https://durhamcollege.ca/wp-content/uploads/ai-hub-header.jpg\" alt=\"DC Logo\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LESSON 10 - Gaussian Naive Bayes\n",
    "\n",
    "## <span style=\"color: green\">OVERVIEW</span>\n",
    "\n",
    "- ref: http://dataaspirant.com/2017/02/20/gaussian-naive-bayes-classifier-implementation-python/\n",
    "\n",
    "Notes:\n",
    "   - Please include the data dictionary in this document so students can understand what the data looks like\n",
    "   - Please include pictures wherever you feel there is too much text to break things up a bit\n",
    "   - Do not include all the steps for the data understanding step. Although it is very important, there is an excessive amount of this data description in the tutorial, only include the ones which you feel are necessary. Make sure you note to the students that it is important to understand the data to the best of their ability before moving forward.\n",
    "   \n",
    "This is a link to the publicly available data set for this lesson. ('adult.data')\n",
    "   \n",
    "   - https://archive.ics.uci.edu/ml/datasets/Adult\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this post, we are going to implement the Naive Bayes classifier in Python using the machine learning library scikit-learn. After which, we are going to use the trained Naive Bayes (supervised classification), model to predict the Census Income.\n",
    "\n",
    "Let's quickly go over the basics of Naive Bayes:\n",
    "\n",
    "Bayes’ theorem is based on <b>conditional probability</b>. The conditional probability helps us calculating the probability that something will happen, given that something else has already happened. Not getting let’s understand with few examples.\n",
    "\n",
    "### Conditional Probability Examples\n",
    "\n",
    "Below are the few examples helps to clearly understand the definition of conditional probability.\n",
    "\n",
    "- Purchasing mac book when you already purchased the iPhone.\n",
    "- Having a refreshing drink when you are in the movie theater.\n",
    "- Buying peanuts when you brought a chilled soft drink.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:350px;\" src=\"https://i2.wp.com/dataaspirant.com/wp-content/uploads/2017/02/Conditional_probability.jpg?w=500\" alt=\"Conditional Probability\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Bayes theorem the naive Bayes classifier works. The naive Bayes classifier assumes all the features are independent to each other. Even if the features depend on each other or upon the existence of the other features. Naive Bayes classifier considers all of these properties to independently contribute to the probability that the user buys the  MacBook.\n",
    "\n",
    "To learn the key concepts related to Naive Bayes. You can read our article on Introduction to Naive Bayes. This will help you understand the core concepts related to Naive Bayes.\n",
    "\n",
    "In the introduction to Naive Bayes post, we discussed three popular Naive Bayes algorithms:\n",
    "\n",
    "- Gaussian Naive Bayes,\n",
    "- Multinomial Naive Bayes.\n",
    "- Bernoulli Naive Bayes.\n",
    "As a continues to the Naive Bayes algorithm article. Now we are going to implement Gaussian Naive Bayes on a “Census Income” dataset.\n",
    "\n",
    "### Gaussian Naive Bayes\n",
    "A Gaussian Naive Bayes algorithm is a special type of NB algorithm. It’s specifically used when the features have continuous values. It’s also assumed that all the features are following a gaussian distribution i.e, normal distribution.\n",
    "\n",
    "### Census Income Dataset\n",
    "Census Income dataset is to predict whether the income of a person >$50K/yr (greater than $50K/yr) or <=$50K/yr. The data was collected by Barry Becker from 1994 Census dataset.\n",
    "\n",
    "This dataset was contributed to UCI repository, and It’s openly available at the link provided at the top of this lesson. The dataset consists of 15 columns of a mix of discrete as well as continuous data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"height: 296px; width: 602px; border-color: #000000; background-color: #ffffff;\" border=\"1 px\">\n",
    "<tbody>\n",
    "<tr style=\"height: 24px;\">\n",
    "<td style=\"width: 37px; height: 24px; text-align: center;\"></td>\n",
    "<td style=\"width: 146px; height: 24px; text-align: center;\"><strong>Variable Name</strong></td>\n",
    "<td style=\"width: 425px; height: 24px; text-align: center;\"><strong>Variable Range</strong></td>\n",
    "</tr>\n",
    "<tr style=\"height: 29px;\">\n",
    "<td style=\"width: 37px; height: 29px; text-align: center;\">1.</td>\n",
    "<td style=\"width: 146px; height: 29px; text-align: center;\">age</td>\n",
    "<td style=\"width: 425px; height: 29px;\">&nbsp;[17 – 90]</td>\n",
    "</tr>\n",
    "<tr style=\"height: 87px;\">\n",
    "<td style=\"width: 37px; height: 87px; text-align: center;\">2.</td>\n",
    "<td style=\"width: 146px; height: 87px; text-align: center;\">workclass</td>\n",
    "<td style=\"width: 425px; height: 87px;\">[‘State-gov’, ‘Self-emp-not-inc’, ‘Private’, ‘Federal-gov’, ‘Local-gov’, ‘Self-emp-inc’, ‘Without-pay’, ‘Never-worked’]</td>\n",
    "</tr>\n",
    "<tr style=\"height: 30px;\">\n",
    "<td style=\"width: 37px; height: 30px; text-align: center;\">3.</td>\n",
    "<td style=\"width: 146px; height: 30px; text-align: center;\">fnlwgt</td>\n",
    "<td style=\"width: 425px; height: 30px;\">[77516- 257302]</td>\n",
    "</tr>\n",
    "<tr style=\"height: 12px;\">\n",
    "<td style=\"width: 37px; height: 12px; text-align: center;\">4.</td>\n",
    "<td style=\"width: 146px; height: 12px; text-align: center;\">education</td>\n",
    "<td style=\"width: 425px; height: 12px;\">\n",
    "<p class=\"\">[‘Bachelors’, ‘HS-grad’, ’11th’, ‘Masters’, ‘9th’, ‘Some-college’, ‘Assoc-acdm’, ‘Assoc-voc’, ‘7th-8th’, ‘Doctorate’, ‘Prof-school’, ‘5th-6th’, ’10th’, ‘1st-4th’, ‘Preschool’, ’12th’]</p>\n",
    "</td>\n",
    "</tr>\n",
    "<tr style=\"height: 31px;\">\n",
    "<td style=\"width: 37px; height: 31px; text-align: center;\">5.</td>\n",
    "<td style=\"width: 146px; height: 31px; text-align: center;\">education_num</td>\n",
    "<td style=\"width: 425px; height: 31px;\">[1 – 16]</td>\n",
    "</tr>\n",
    "<tr style=\"height: 24px;\">\n",
    "<td style=\"width: 37px; height: 24px; text-align: center;\">6.</td>\n",
    "<td style=\"width: 146px; height: 24px; text-align: center;\">marital_status</td>\n",
    "<td style=\"width: 425px; height: 24px;\">\n",
    "<p class=\"\">[‘Never-married’, ‘Married-civ-spouse’, ‘Divorced’, ‘Married-spouse-absent’, ‘Separated’, ‘Married-AF-spouse’, ‘Widowed’]</p>\n",
    "</td>\n",
    "</tr>\n",
    "<tr style=\"height: 24px;\">\n",
    "<td style=\"width: 37px; height: 24px; text-align: center;\">7.</td>\n",
    "<td style=\"width: 146px; height: 24px; text-align: center;\">occupation</td>\n",
    "<td style=\"width: 425px; height: 24px;\">\n",
    "<p class=\"\">[‘Adm-clerical’, ‘Exec-managerial’, ‘Handlers-cleaners’, ‘Prof-specialty’, ‘Other-service’, ‘Sales’, ‘Craft-repair’, ‘Transport-moving’, ‘Farming-fishing’, ‘Machine-op-inspct’, ‘Tech-support’, ‘Protective-serv’, ‘Armed-Forces’, ‘Priv-house-serv’]</p>\n",
    "</td>\n",
    "</tr>\n",
    "<tr style=\"height: 24px;\">\n",
    "<td style=\"width: 37px; height: 24px; text-align: center;\">8.</td>\n",
    "<td style=\"width: 146px; height: 24px; text-align: center;\">relationship</td>\n",
    "<td style=\"width: 425px; height: 24px;\">\n",
    "<p class=\"\">[‘Not-in-family’, ‘Husband’, ‘Wife’, ‘Own-child’, ‘Unmarried’, ‘Other-relative’]</p>\n",
    "</td>\n",
    "</tr>\n",
    "<tr style=\"height: 24px;\">\n",
    "<td style=\"width: 37px; height: 24px; text-align: center;\">9.</td>\n",
    "<td style=\"width: 146px; height: 24px; text-align: center;\">race</td>\n",
    "<td style=\"width: 425px; height: 24px;\">\n",
    "<p class=\"\">[‘White’, ‘Black’, ‘Asian-Pac-Islander’, ‘Amer-Indian-Eskimo’, ‘Other’]</p>\n",
    "</td>\n",
    "</tr>\n",
    "<tr style=\"height: 24px;\">\n",
    "<td style=\"width: 37px; height: 24px; text-align: center;\">10.</td>\n",
    "<td style=\"width: 146px; height: 24px; text-align: center;\">sex</td>\n",
    "<td style=\"width: 425px; height: 24px;\">\n",
    "<p class=\"\">[‘Male’, ‘Female’]</p>\n",
    "</td>\n",
    "</tr>\n",
    "<tr style=\"height: 29px;\">\n",
    "<td style=\"width: 37px; height: 29px; text-align: center;\">11.</td>\n",
    "<td style=\"width: 146px; height: 29px; text-align: center;\">capital_gain</td>\n",
    "<td style=\"width: 425px; height: 29px;\">[0 – 99999]</td>\n",
    "</tr>\n",
    "<tr style=\"height: 28px;\">\n",
    "<td style=\"width: 37px; height: 28px; text-align: center;\">12.</td>\n",
    "<td style=\"width: 146px; height: 28px; text-align: center;\">capital_loss</td>\n",
    "<td style=\"width: 425px; height: 28px;\">[0 –&nbsp;4356]</td>\n",
    "</tr>\n",
    "<tr style=\"height: 28px;\">\n",
    "<td style=\"width: 37px; height: 28px; text-align: center;\">13.</td>\n",
    "<td style=\"width: 146px; height: 28px; text-align: center;\">hours_per_week</td>\n",
    "<td style=\"width: 425px; height: 28px;\">[1 – 99]</td>\n",
    "</tr>\n",
    "<tr style=\"height: 24px;\">\n",
    "<td style=\"width: 37px; height: 24px; text-align: center;\">14.</td>\n",
    "<td style=\"width: 146px; height: 24px; text-align: center;\">native_country</td>\n",
    "<td style=\"width: 425px; height: 24px;\">\n",
    "<p class=\"\">[‘United-States’, ‘Cuba’, ‘Jamaica’, ‘India’, ‘Mexico’, ‘South’, ‘Puerto-Rico’, ‘Honduras’, ‘England’, ‘Canada’, ‘Germany’, ‘Iran’, ‘Philippines’, ‘Italy’, ‘Poland’, ‘Columbia’, ‘Cambodia’, ‘Thailand’, ‘Ecuador’, ‘Laos’, ‘Taiwan’, ‘Haiti’, ‘Portugal’, ‘Dominican-Republic’, ‘El-Salvador’, ‘France’, ‘Guatemala’, ‘China’, ‘Japan’, ‘Yugoslavia’, ‘Peru’, ‘Outlying-US(Guam-USVI-etc)’, ‘Scotland’, ‘Trinadad&amp;Tobago’, ‘Greece’, ‘Nicaragua’, ‘Vietnam’, ‘Hong’, ‘Ireland’, ‘Hungary’, ‘Holand-Netherlands’]</p>\n",
    "</td>\n",
    "</tr>\n",
    "<tr style=\"height: 13.5938px;\">\n",
    "<td style=\"width: 37px; height: 13.5938px; text-align: center;\">15.</td>\n",
    "<td style=\"width: 146px; height: 13.5938px; text-align: center;\">income</td>\n",
    "<td style=\"width: 425px; height: 13.5938px;\">\n",
    "<p class=\"\">&nbsp;[‘&lt;=50K’, ‘&gt;50K’]</p>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final target variable consists of two values: ‘<=50K” & ‘>50K’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Gaussian NB on Census Income dataset\n",
    "### Importing Python Machine Learning Libraries\n",
    "We need to import pandas, numpy and sklearn libraries. From sklearn, we need to import preprocessing modules like Imputer. The Imputer package helps to impute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Required Python Machine learning Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For preprocessing the data\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# To split the dataset into train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To model the Gaussian Navie Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# To calculate the accuracy score of the model\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>train_test_split</b> module is for splitting the dataset into training and testing set. The <b>accuracy_score</b> module will be used for calculating the accuracy of our Gaussian Naive Bayes algorithm.\n",
    "\n",
    "### Data Import\n",
    "For importing the census data, we are using pandas <b>read_csv()</b> method. This method is a very simple and fast method for importing data.\n",
    "\n",
    "We are passing four parameters. The ‘adult.data’ parameter is the <b>file name</b>. The header parameter is for giving details to pandas that whether the first row of data consists of headers or not. In our dataset, there is no header. <i>So, we are passing None</i>.\n",
    "\n",
    "The delimiter parameter is for giving the information the delimiter that is separating the data. Here, we are using ‘ *, *’ delimiter. This delimiter is to show delete the spaces before and after the data values. This is very helpful when there is inconsistency in spaces used with data values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Dataset\n",
    "\n",
    "adult_df = pd.read_csv('adult.data.txt',\n",
    "                       header = None, delimiter=' *, *', engine='python')\n",
    "\n",
    "adult_df = pd.read_csv('adult.data.txt',\n",
    "                       header = None, delimiter=' *, *', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let’s add headers to our dataframe. The below code snippet can be used to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding Headers\n",
    "adult_df.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "                    'marital_status', 'occupation', 'relationship',\n",
    "                    'race', 'sex', 'capital_gain', 'capital_loss',\n",
    "                    'hours_per_week', 'native_country', 'income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "Let’s try to test whether there is any null value in our dataset or not. We can do this using isnull() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education_num     0\n",
       "marital_status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "capital_gain      0\n",
       "capital_loss      0\n",
       "hours_per_week    0\n",
       "native_country    0\n",
       "income            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding Null values\n",
    "adult_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output shows that there is no “null” value in our dataset.\n",
    "\n",
    "Let’s try to test whether any categorical attribute contains a “?” in it or not. At times there exists “?” or ” ” in place of missing values. Using the below code snippet we are going to test whether our adult_df data frame consists of categorical variables with values as “?”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass : 1836\n",
      "education : 0\n",
      "marital_status : 0\n",
      "occupation : 1843\n",
      "relationship : 0\n",
      "race : 0\n",
      "sex : 0\n",
      "native_country : 583\n",
      "income : 0\n"
     ]
    }
   ],
   "source": [
    "for value in ['workclass', 'education', 'marital_status', 'occupation', 'relationship','race', 'sex', 'native_country', 'income']:\n",
    "    print(value, \":\", sum(adult_df[value] == '?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the above code snippet shows that there are 1836 missing values in workclass attribute. 1843 missing values in occupation attribute and 583 values in native_country attribute.\n",
    "\n",
    "### Data preprocessing\n",
    "For preprocessing, we are going to make a duplicate copy of our original dataframe.We are duplicating adult_df to adult_df_rev dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adult_df_rev = adult_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to perform some imputation for missing values. Before doing that, we need some summary statistics of our dataframe. For this, we can use describe() method. It can be used to generate various summary statistics, excluding NaN values.\n",
    "\n",
    "We are passing an “include” parameter with value as “all”, this is used to specify that. we want summary statistics of all the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561</td>\n",
       "      <td>3.256100e+04</td>\n",
       "      <td>32561</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561</td>\n",
       "      <td>32561</td>\n",
       "      <td>32561</td>\n",
       "      <td>32561</td>\n",
       "      <td>32561</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561.000000</td>\n",
       "      <td>32561</td>\n",
       "      <td>32561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Private</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14976</td>\n",
       "      <td>4140</td>\n",
       "      <td>13193</td>\n",
       "      <td>27816</td>\n",
       "      <td>21790</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29170</td>\n",
       "      <td>24720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.581647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897784e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.080679</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1077.648844</td>\n",
       "      <td>87.303830</td>\n",
       "      <td>40.437456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.640433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.055500e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.572720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7385.292085</td>\n",
       "      <td>402.960219</td>\n",
       "      <td>12.347429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.228500e+04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.178270e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.783560e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.370510e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.484705e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99999.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 age workclass        fnlwgt education  education_num  \\\n",
       "count   32561.000000     32561  3.256100e+04     32561   32561.000000   \n",
       "unique           NaN         9           NaN        16            NaN   \n",
       "top              NaN   Private           NaN   HS-grad            NaN   \n",
       "freq             NaN     22696           NaN     10501            NaN   \n",
       "mean       38.581647       NaN  1.897784e+05       NaN      10.080679   \n",
       "std        13.640433       NaN  1.055500e+05       NaN       2.572720   \n",
       "min        17.000000       NaN  1.228500e+04       NaN       1.000000   \n",
       "25%        28.000000       NaN  1.178270e+05       NaN       9.000000   \n",
       "50%        37.000000       NaN  1.783560e+05       NaN      10.000000   \n",
       "75%        48.000000       NaN  2.370510e+05       NaN      12.000000   \n",
       "max        90.000000       NaN  1.484705e+06       NaN      16.000000   \n",
       "\n",
       "            marital_status      occupation relationship   race    sex  \\\n",
       "count                32561           32561        32561  32561  32561   \n",
       "unique                   7              15            6      5      2   \n",
       "top     Married-civ-spouse  Prof-specialty      Husband  White   Male   \n",
       "freq                 14976            4140        13193  27816  21790   \n",
       "mean                   NaN             NaN          NaN    NaN    NaN   \n",
       "std                    NaN             NaN          NaN    NaN    NaN   \n",
       "min                    NaN             NaN          NaN    NaN    NaN   \n",
       "25%                    NaN             NaN          NaN    NaN    NaN   \n",
       "50%                    NaN             NaN          NaN    NaN    NaN   \n",
       "75%                    NaN             NaN          NaN    NaN    NaN   \n",
       "max                    NaN             NaN          NaN    NaN    NaN   \n",
       "\n",
       "        capital_gain  capital_loss  hours_per_week native_country income  \n",
       "count   32561.000000  32561.000000    32561.000000          32561  32561  \n",
       "unique           NaN           NaN             NaN             42      2  \n",
       "top              NaN           NaN             NaN  United-States  <=50K  \n",
       "freq             NaN           NaN             NaN          29170  24720  \n",
       "mean     1077.648844     87.303830       40.437456            NaN    NaN  \n",
       "std      7385.292085    402.960219       12.347429            NaN    NaN  \n",
       "min         0.000000      0.000000        1.000000            NaN    NaN  \n",
       "25%         0.000000      0.000000       40.000000            NaN    NaN  \n",
       "50%         0.000000      0.000000       40.000000            NaN    NaN  \n",
       "75%         0.000000      0.000000       45.000000            NaN    NaN  \n",
       "max     99999.000000   4356.000000       99.000000            NaN    NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset basic statistics\n",
    "\n",
    "adult_df_rev.describe(include= 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You check the basic statistics about the dataset after running the above command. You can spend some time here to get in details about each and ever stats provided.\n",
    "### Data Imputation Step\n",
    "Now, it’s time to impute the missing values. Some of our categorical values have missing values i.e, “?”. We are going to replace the “?” with the above describe methods top row’s value. For example, we are going to replace the “?” values of workplace attribute with “Private” value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in ['workclass', 'education',\n",
    "          'marital_status', 'occupation',\n",
    "          'relationship','race', 'sex',\n",
    "          'native_country', 'income']:\n",
    "    adult_df_rev[value].replace(['?'], [adult_df_rev.describe(include='all')[value][2]],\n",
    "                                inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have performed data imputation step. 🙂\n",
    "You can check changes in dataframe by printing <i>adult_df_rev</i>\n",
    "\n",
    "For naive Bayes, we need to convert all the data values in one format. We are going to encode all the labels with the value between 0 and n_classes-1.\n",
    "### One-Hot Encoder\n",
    "For implementing this, we are going to use LabelEncoder of scikit learn library. For encoding, we can also use the One-Hot encoder. It encodes the data into binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "workclass_cat = le.fit_transform(adult_df.workclass)\n",
    "education_cat = le.fit_transform(adult_df.education)\n",
    "marital_cat   = le.fit_transform(adult_df.marital_status)\n",
    "occupation_cat = le.fit_transform(adult_df.occupation)\n",
    "relationship_cat = le.fit_transform(adult_df.relationship)\n",
    "race_cat = le.fit_transform(adult_df.race)\n",
    "sex_cat = le.fit_transform(adult_df.sex)\n",
    "native_country_cat = le.fit_transform(adult_df.native_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize the encoded categorical columns\n",
    "adult_df_rev['workclass_cat'] = workclass_cat\n",
    "adult_df_rev['education_cat'] = education_cat\n",
    "adult_df_rev['marital_cat'] = marital_cat\n",
    "adult_df_rev['occupation_cat'] = occupation_cat\n",
    "adult_df_rev['relationship_cat'] = relationship_cat\n",
    "adult_df_rev['race_cat'] = race_cat\n",
    "adult_df_rev['sex_cat'] = sex_cat\n",
    "adult_df_rev['native_country_cat'] = native_country_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop the old categorical columns from dataframe\n",
    "dummy_fields = ['workclass', 'education', 'marital_status', \n",
    "                  'occupation', 'relationship', 'race',\n",
    "                  'sex', 'native_country']\n",
    "adult_df_rev = adult_df_rev.drop(dummy_fields, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above code snippets, we have created multiple categorical columns like “marital_cat”, “race_cat” etc. You can see the top 6 lines of the dataframe using adult_df_rev.head()\n",
    "By printing the  adult_df_rev.head()  result. You will be able to see that all the columns should be reindexed. They are not in proper order. For reindexing the columns, you can use the code snippet provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass_cat</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education_cat</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_cat</th>\n",
       "      <th>occupation_cat</th>\n",
       "      <th>relationship_cat</th>\n",
       "      <th>race_cat</th>\n",
       "      <th>sex_cat</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country_cat</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>77516</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass_cat  fnlwgt  education_cat  education_num  marital_cat  \\\n",
       "0   39              6   77516              9             13            4   \n",
       "\n",
       "   occupation_cat  relationship_cat  race_cat  sex_cat  capital_gain  \\\n",
       "0               0                 1         4        1          2174   \n",
       "\n",
       "   capital_loss  hours_per_week  native_country_cat income  \n",
       "0             0              40                  38  <=50K  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_df_rev = adult_df_rev.reindex_axis(['age', 'workclass_cat', 'fnlwgt', 'education_cat',\n",
    "                                    'education_num', 'marital_cat', 'occupation_cat',\n",
    "                                    'relationship_cat', 'race_cat', 'sex_cat', 'capital_gain',\n",
    "                                    'capital_loss', 'hours_per_week', 'native_country_cat', \n",
    "                                    'income'], axis= 1)\n",
    " \n",
    "adult_df_rev.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output to the above code snippet will show you that all the columns are reindexed properly. I have passed the list of column names as a parameter and axis=1 for reindexing the columns.\n",
    "\n",
    "### Standardization of Data\n",
    "All the data values of our dataframe are numeric. Now, we need to convert them on a single scale. We can standardize the values.  We can use the below formula for standardization.\n",
    "\n",
    "<img src=\"http://s0.wp.com/latex.php?latex=%7Bx%7D_i+%3D+%5Cfrac%7B%7Bx%7D_i+-+mean%28x%29%7D+%7B%5Csigma%28x%29%7D&bg=ffffff&fg=000&s=0\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Standardization of Data\n",
    "\n",
    "num_features = ['age', 'workclass_cat', 'fnlwgt', 'education_cat', 'education_num',\n",
    "                'marital_cat', 'occupation_cat', 'relationship_cat', 'race_cat',\n",
    "                'sex_cat', 'capital_gain', 'capital_loss', 'hours_per_week',\n",
    "                'native_country_cat']\n",
    "\n",
    "scaled_features = {}\n",
    "for each in num_features:\n",
    "    mean, std = adult_df_rev[each].mean(), adult_df_rev[each].std()\n",
    "    scaled_features[each] = [mean, std]\n",
    "    adult_df_rev.loc[:, each] = (adult_df_rev[each] - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have converted our data values into standardized values. You can print and check the output of dataframe.\n",
    "\n",
    "### Data Slicing\n",
    "Let’s split the data into training and test set. We can easily perform this step using sklearn’s train_test_split() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = adult_df_rev.values[:,:14]\n",
    "target = adult_df_rev.values[:,14]\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size = 0.33, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using above code snippet, we have divided the data into features and target set. The feature set consists of 14 columns i.e, predictor variables and target set consists of 1 column with class values.\n",
    "\n",
    "The features_train & target_train consists of training data and the features_test & target_test consists of testing data.\n",
    "\n",
    "### Gaussian Naive Bayes Implementation\n",
    "After completing the data preprocessing. it’s time to implement machine learning algorithm on it. We are going to use sklearn’s GaussianNB module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(features_train, target_train)\n",
    "target_pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built a GaussianNB classifier. The classifier is trained using training data. We can use fit() method for training it. After building a classifier, our model is ready to make predictions. We can use predict() method with test set features as its parameters.\n",
    "\n",
    "### Accuracy of our Gaussian Naive Bayes model\n",
    "It’s time to test the quality of our model. We have made some predictions. Let’s compare the model’s prediction with actual target values for the test set. By following this method, we are going to calculate the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80141447980643965"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(target_test, target_pred, normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Our model is returning an <b>accuracy of 80%</b>. This is not bad with a simple implementation, and could easily be made more effective. You can create random test datasets and test the model to get know how well the trained Gaussian Naive Bayes model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
