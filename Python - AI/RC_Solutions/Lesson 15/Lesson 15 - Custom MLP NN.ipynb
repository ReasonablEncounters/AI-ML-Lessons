{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: #8e44ad\">Challenge â†’</span>\n",
    "\n",
    "- Modify this Neural Network to be within a Python Class. This is so you can instantiate this Neural Network whenever you'd like.\n",
    "\n",
    "    * The class should have a train function which is essentially the code given above. It should take an epoch paramater which is the number of epochs the network will train for (it is 10,000 in the example above). \n",
    "    * The class should have a test function, which will run an example through the network and give an output. Remember, the testing of the function is just the forward pass of the network. You will have to code some logic to give you a digestable output that makes sense. \n",
    "    * The neural network should take a parameter upon instantiation that dictates the number of hidden layers. This portion of the challenge will require the most thought and work. Feel free to work with partners on this part as it is very important you understand how this is done. \n",
    "\n",
    "**Hint:** Adding multiple layers to the network will be tough. Just remember that your input layer goes to your hidden layer and your hidden layer goes to your output layer. You will have to inject the capability to modularly make more layers using lists of values. All the math for these layers can be reused from the code above.\n",
    "\n",
    "**Hint:** Draw out your architecture on a piece of paper to visualize it before actually jumping into it. This will help.\n",
    "\n",
    "**Hint:** You can verify your network is working properly by training it and observing that your loss is indeed going down instead of up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transfer functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid\n",
    "def dsigmoid(y):\n",
    "    return y * (1.0 - y)\n",
    "\n",
    "# using softmax as output layer is recommended for classification where outputs are mutually exclusive\n",
    "def softmax(w):\n",
    "    exp_scores = np.exp(w)\n",
    "    # axis is 1 to sum across rows\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "# using tanh over logistic sigmoid for the hidden layer is recommended   \n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "# derivative for tanh sigmoid\n",
    "def dtanh(y):\n",
    "    return 1 - y*y\n",
    "\n",
    "# Generate some labels for out data - this will be the true data\n",
    "def generate_labels(samples, classes):\n",
    "    y = np.array([])\n",
    "\n",
    "    for i in range(classes):\n",
    "        y = np.append(y, [i]*samples)\n",
    "\n",
    "    np.random.shuffle(y)\n",
    "    return np.vstack(y[:samples]).astype(int)\n",
    "\n",
    "# Column normalize with negatives\n",
    "def normalize_with_negatives(x):\n",
    "    x = np.array(x)\n",
    "    # ptp(0) returns the \"peak-to-peak\" (i.e. the range, max - min) along axis 0.\n",
    "    # this is necessary to handle negative values and guaranteed the minimum value in each column is 0\n",
    "    x_normed = (x - x.min(0)) / x.ptp(0)\n",
    "    return x_normed\n",
    "\n",
    "def normalize(x):\n",
    "    x = np.array(x)\n",
    "    x_normed = x / x.max(axis=0)\n",
    "    return x_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(object):\n",
    "    \n",
    "    # define the class constructor\n",
    "    def __init__(self,\n",
    "                 X,\n",
    "                 y,\n",
    "                 input_dim,\n",
    "                 hidden_dims,\n",
    "                 output_dim,\n",
    "                 learning_rate,\n",
    "                 regularization\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param X: training samples\n",
    "        :param y: training labels\n",
    "        \n",
    "        :param input_dim:   number of input neurons\n",
    "        :param hidden_dims: number of hidden neurons in an array\n",
    "        :param output_dim:  number of output neurons\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize parameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.samples = X.shape[0]\n",
    "        self.regularization = regularization;\n",
    "        self.learning_rate = learning_rate;\n",
    "        self.print_loss=True;\n",
    "        self.print_loss_iteration=1000;\n",
    "        self.reg_lambda = 0.01;\n",
    "\n",
    "        # training data and labels\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        # +1 as we need n layers - 1 weights and biases\n",
    "        self.weights = [[] for i in range(len(self.hidden_dims)+1)]        \n",
    "        self.biases = [[] for i in range(len(self.hidden_dims)+1)]    \n",
    "\n",
    "        # preactivation layer arrays\n",
    "        self.z = [[] for i in range(len(self.hidden_dims)+1)] \n",
    "        # activation layer arrays\n",
    "        self.a = [[] for i in range(len(self.hidden_dims)+1)]\n",
    "        # deltas arrays\n",
    "        self.deltas = [[] for i in range(len(self.hidden_dims)+1)]\n",
    "        self.deltaWeights = [[] for i in range(len(self.hidden_dims)+1)]\n",
    "        self.deltaBiases = [[] for i in range(len(self.hidden_dims)+1)]\n",
    "\n",
    "        # Initialize the parameters to random values. We need to learn these.\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # the reason we are going to the size od hidden_dims + 1 is because we need \n",
    "        # weights and biases for the total number of layers - 1\n",
    "        for i in range(len(self.hidden_dims)+1):\n",
    "            np.random.seed(np.random.randint(99))\n",
    "            if i == 0:\n",
    "                # do stuff for first layer\n",
    "                self.weights[i] = np.random.randn(self.input_dim, self.hidden_dims[i])# / np.sqrt(self.input_dim)\n",
    "                self.biases[i] = np.zeros((1, self.hidden_dims[i]))\n",
    "\n",
    "            elif i == len(self.hidden_dims):\n",
    "                # do stuff for last layer\n",
    "                self.weights[i] = np.random.randn(self.hidden_dims[i-1], self.output_dim)# / np.sqrt(self.hidden_dims[i-1])\n",
    "                self.biases[i] = np.zeros((1, self.output_dim))\n",
    "\n",
    "            else:\n",
    "                # do stuff for every other middle juicy deep layer\n",
    "                self.weights[i] = np.random.randn(self.hidden_dims[i-1], self.hidden_dims[i])# / np.sqrt(self.hidden_dims[i-1])\n",
    "                self.biases[i] = np.zeros((1, self.hidden_dims[i]))\n",
    "\n",
    "\n",
    "        # This is what we return at the end\n",
    "        self.model = {}\n",
    "\n",
    "        \n",
    "    # calculate the accuracy of your training data\n",
    "    # :param predictions: your actual predictions (1d array)\n",
    "    # :param labels: real labels (1d array)\n",
    "    def calculate_accuracy(self, predictions, labels):\n",
    "        \n",
    "        accuracy = 0\n",
    "        for i in range(len(predictions)):\n",
    "            \n",
    "            if predictions[i] == labels[i]:\n",
    "                accuracy += 1\n",
    "                \n",
    "        return accuracy / len(predictions)\n",
    "        \n",
    "        \n",
    "    # feed forward function since it is being called a lot\n",
    "    def feedforward(self, X):\n",
    "        \n",
    "        # Forward propagation to calculate our predictions\n",
    "        probs = []\n",
    "        \n",
    "        for i in range(len(self.hidden_dims) + 1):\n",
    "            if i == 0:\n",
    "                # do stuff for first layer\n",
    "                self.z[i] = X.dot(self.weights[i]) + self.biases[i]\n",
    "                self.a[i] = np.tanh(self.z[i])\n",
    "                \n",
    "            elif i == len(self.hidden_dims):\n",
    "                # do stuff for last layer\n",
    "                self.z[i] = self.a[i-1].dot(self.weights[i]) + self.biases[i]\n",
    "                # calculate the probabilities of the output layer\n",
    "                probs = softmax(self.z[i])\n",
    "                \n",
    "            else:\n",
    "                # do stuff for every other middle juicy deep layer\n",
    "                self.z[i] = self.a[i-1].dot(self.weights[i]) + self.biases[i]\n",
    "                self.a[i] = np.tanh(self.z[i])\n",
    "                \n",
    "        return probs\n",
    "    \n",
    "    # Helpers function to evaluate the total loss on the dataset\n",
    "    def calculate_loss(self, model):\n",
    "        \n",
    "        # feed forward to get our predictions\n",
    "        probs = self.feedforward(self.X)            \n",
    "        \n",
    "        correct_logprobs = -np.log(probs[range(self.samples), np.matrix(self.y).A1.astype(int)])\n",
    "        data_loss = np.sum(correct_logprobs)\n",
    "        \n",
    "        # Add regulatization term to loss (optional)\n",
    "        if self.regularization == True:\n",
    "            sum = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                # add the sum to a cumulative variable for use later to calculate data loss\n",
    "                sum += np.sum(np.square(self.weights[i]))\n",
    "\n",
    "            data_loss += self.reg_lambda/2 * sum\n",
    "        \n",
    "        return 1./self.samples*data_loss\n",
    "    \n",
    "    \n",
    "     # Helper function to predict an output (0 or 1)\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # feed forward to get our predictions\n",
    "        probs = self.feedforward(X)\n",
    "        \n",
    "        return np.argmax(probs, axis=1)\n",
    "        \n",
    "    # Define a training method that accepts a number of epoch iterations to run\n",
    "    def train(self, epoch_iterations):\n",
    "        \n",
    "        self.samples = self.X.shape[0] # 1500 samples\n",
    "        self.features = self.X.shape[1] # 2 features\n",
    "        classes = 3\n",
    "\n",
    "        alpha = 10e-6\n",
    "        costs = []\n",
    "        \n",
    "        self.ei = epoch_iterations\n",
    "        \n",
    "        for j in range(self.ei + 1):\n",
    "            # forward pass\n",
    "            probs = self.feedforward(self.X)\n",
    "\n",
    "            # BACKPROPAGATION\n",
    "\n",
    "            # let's go in the opposite direction this time ;)\n",
    "            for i in reversed(range(len(self.hidden_dims) + 1)):\n",
    "                if i == len(self.hidden_dims):\n",
    "                    # do stuff for first delta (which is actually the last in the array)\n",
    "                    self.deltas[i] = probs\n",
    "\n",
    "                    self.deltas[i][range(len(self.y)), np.matrix(self.y).A1.astype(int)] -= 1 \n",
    "                    self.deltaWeights[i] = (self.a[i-1].T).dot(self.deltas[i])\n",
    "                    self.deltaBiases[i] = np.sum(self.deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "                elif i == 0:\n",
    "                    # do stuff for first layer\n",
    "                    self.deltas[i] = self.deltas[i+1].dot(self.weights[i+1].T) * (1 - np.power(self.a[i], 2))\n",
    "                    self.deltaWeights[i] = np.dot(self.X.T, self.deltas[i])\n",
    "                    self.deltaBiases[i] = np.sum(self.deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "    #                 elif i == 0:\n",
    "    #                     # do nothing\n",
    "    #                     print(\"end of backprop loop\",i)\n",
    "\n",
    "                elif i != 0:\n",
    "                    # do stuff for every other middle juicy deep layer\n",
    "                    self.deltas[i] = self.deltas[i+1].dot(self.weights[i+1].T) * (1 - np.power(self.a[i], 2))\n",
    "                    self.deltaWeights[i] = (self.a[i-1].T).dot(self.deltas[i])\n",
    "                    self.deltaBiases[i] = np.sum(self.deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "            if self.regularization == True:\n",
    "                for i in range(len(self.deltaWeights)):\n",
    "                    self.deltaWeights[i] += self.reg_lambda * self.weights[i]\n",
    "\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] += -self.learning_rate * self.deltaWeights[i]\n",
    "                self.biases[i] += -self.learning_rate * self.deltaBiases[i]\n",
    "\n",
    "            # Assign new parameters to the model\n",
    "            model = { 'weights': self.weights, 'biases': self.biases}\n",
    "\n",
    "            # Optionally print the loss.\n",
    "            # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "            if self.print_loss and j % self.print_loss_iteration == 0:\n",
    "                print (\"Loss after iteration %i: %f\" %(j, self.calculate_loss(model)))\n",
    "\n",
    "        return model\n",
    "\n",
    "    # DISPLAY THE COST        \n",
    "    #plt.plot(costs)\n",
    "    #plt.show()\n",
    "        \n",
    "    # Define a testing method that accepts an array of test data\n",
    "    def test(self, X_test, y_test):\n",
    "        \n",
    "        # calculate loss\n",
    "        nn.calculate_loss({})\n",
    "        \n",
    "        # forward pass (over pre-split test data)\n",
    "        probs = self.feedforward(X_test)\n",
    "        \n",
    "        # magic\n",
    "        predictions = nn.predict(X_test)\n",
    "        np.set_printoptions(threshold=np.inf)\n",
    "        print(np.array(predictions))\n",
    "        \n",
    "        labels = y_test.astype(int).squeeze()\n",
    "        labels\n",
    "        accuracy = nn.calculate_accuracy(predictions, labels)\n",
    "        accuracy = accuracy * 100\n",
    "        # output accuracy\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dynamic Gaussian clouds each holding 500 points\n",
    "X1 = np.random.randn(200, 2) + np.array([0, -2])\n",
    "X2 = np.random.randn(200, 2) + np.array([2, 2])\n",
    "X3 = np.random.randn(200, 2) + np.array([-2, 2])\n",
    "\n",
    "# put them all in a big matrix\n",
    "X = np.vstack([X1, X2, X3])\n",
    "data = X\n",
    "X = np.array(data[:, :1])\n",
    "y = np.array(data[:, [1]])\n",
    "\n",
    "# generate the one-hot-encodings\n",
    "# remember: these labels will be the corresponding classes to the data we generated above.\n",
    "labels = np.array([0]*200 + [1]*200 + [2]*200)\n",
    "T = np.zeros((600, 3))\n",
    "for i in range(600):\n",
    "    T[i, labels[i]] = 1\n",
    "\n",
    "test_train_split = 0.7\n",
    "y_train = y[:int(np.rint(y.shape[0]*test_train_split))]\n",
    "X_train = X[:int(np.rint(X.shape[0]*test_train_split))]\n",
    "y_test = y[int(np.rint(y.shape[0]*test_train_split)):]\n",
    "X_test = X[int(np.rint(X.shape[0]*test_train_split)):]\n",
    "\n",
    "nn = NeuralNet(X_train, \n",
    "               y_train, \n",
    "               X_train.shape[1], \n",
    "               [1, 8, 1], \n",
    "               len(np.unique(y)),\n",
    "               learning_rate = 0.001,\n",
    "               regularization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 1.681307\n",
      "Loss after iteration 1000: 1.681203\n",
      "Loss after iteration 2000: 1.681104\n",
      "Loss after iteration 3000: 1.681009\n"
     ]
    }
   ],
   "source": [
    "# START TRAINING HERE\n",
    "nn.train(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 598   0   2 598   2   2   0 598 598   0   2 599   2   2   2   2   2\n",
      "   2   2   2 598   0   0 598   0   0   2   2 599   2   0   2   0 599 598\n",
      "   2 598 599   2 598   2   0   0   0   0   0   2   2   2 599   2   0   0\n",
      "   2   2 599   2   0   2 598   0   2   0   2 599 598   0 598   2   2   2\n",
      "   0   0 598   2 598 599 599 598   2   2 598   0   2   0   2 599   2 598\n",
      "   0 598 599 598   0 598   0 599 599 598   2 598 598   0 598   0   2   0\n",
      "   2   2 599   0 598   2 598   2   2   2 598   2   2   0   2   2   2   0\n",
      "   2   2 599 599   2 599   2 598   2   2 598   2 599   2   2 599   2   0\n",
      " 599   2   2   0   2   0   2   0 598   2 599   2   0   2   0 599   2   2\n",
      "   0 598   2   2   0   2 598   2   2 598   2   2 599   2   0 599   0   2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21.666666666666668"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# START TESTING HERE\n",
    "nn.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
