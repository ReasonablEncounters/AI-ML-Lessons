{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transfer functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid\n",
    "def dsigmoid(y):\n",
    "    return y * (1.0 - y)\n",
    "\n",
    "# using softmax as output layer is recommended for classification where outputs are mutually exclusive\n",
    "def softmax(w):\n",
    "    exp_scores = np.exp(w)\n",
    "    # axis is 1 to sum across rows\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "# using tanh over logistic sigmoid for the hidden layer is recommended   \n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "# derivative for tanh sigmoid\n",
    "def dtanh(y):\n",
    "    return 1 - y*y\n",
    "\n",
    "# Generate some labels for out data - this will be the true data\n",
    "def generate_labels(samples, classes):\n",
    "    y = np.array([])\n",
    "\n",
    "    for i in range(classes):\n",
    "        y = np.append(y, [i]*samples)\n",
    "\n",
    "    np.random.shuffle(y)\n",
    "    return np.vstack(y[:samples]).astype(int)\n",
    "\n",
    "# Column normalize with negatives\n",
    "def normalize_with_negatives(x):\n",
    "    x = np.array(x)\n",
    "    # ptp(0) returns the \"peak-to-peak\" (i.e. the range, max - min) along axis 0.\n",
    "    # this is necessary to handle negative values and guaranteed the minimum value in each column is 0\n",
    "    x_normed = (x - x.min(0)) / x.ptp(0)\n",
    "    return x_normed\n",
    "\n",
    "def normalize(x):\n",
    "    x = np.array(x)\n",
    "    x_normed = x / x.max(axis=0)\n",
    "    return x_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP_NeuralNetwork(object):\n",
    "    \n",
    "    # Class constructor\n",
    "    def __init__(self, \n",
    "                 X, \n",
    "                 y, \n",
    "                 input_dim, \n",
    "                 hidden_dims, \n",
    "                 output_dim, \n",
    "                 iterations = 50, \n",
    "                 learning_rate = 0.001,\n",
    "                 regularization = True,\n",
    "                 regularization_rate = 0.01):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        :param X: training samples\n",
    "        :param y: training labels\n",
    "        \n",
    "        :param input_dim: number of input neurons\n",
    "        :param hidden_dims: number of hidden neurons in an array\n",
    "        :param output_dim: number of output neurons\n",
    "        \n",
    "        :param iterations: how many epochs\n",
    "        :param learning_rate: initial learning rate\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize parameters\n",
    "        self.regularization = regularization\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = regularization_rate\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.samples = X.shape[0]\n",
    "        \n",
    "        self.hidden_dims = hidden_dims\n",
    "        \n",
    "        # training input and labels\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # +1 as we need n layers - 1 weights and biases\n",
    "        self.weights = [[] for i in range(len(self.hidden_dims)+1)]        \n",
    "        self.biases = [[] for i in range(len(self.hidden_dims)+1)]    \n",
    "        \n",
    "        # preactivation layer arrays\n",
    "        self.z = [[] for i in range(len(self.hidden_dims)+1)] \n",
    "        # activation layer arrays\n",
    "        self.a = [[] for i in range(len(self.hidden_dims)+1)]\n",
    "        # deltas arrays\n",
    "        self.deltas = [[] for i in range(len(self.hidden_dims)+1)]\n",
    "        self.deltaWeights = [[] for i in range(len(self.hidden_dims)+1)]\n",
    "        self.deltaBiases = [[] for i in range(len(self.hidden_dims)+1)]\n",
    "\n",
    "        \n",
    "        # Initialize the parameters to random values. We need to learn these.\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        # the reason we are using the size of hidden_dims + 1 is because we need \n",
    "        # weights and biases for the total number of layers - 1\n",
    "        for i in range(len(self.hidden_dims)+1):\n",
    "            np.random.seed(np.random.randint(99))\n",
    "            if i == 0:\n",
    "                # do stuff for first layer\n",
    "                self.weights[i] = np.random.randn(self.input_dim, self.hidden_dims[i])# / np.sqrt(self.input_dim)\n",
    "                self.biases[i] = np.zeros((1, self.hidden_dims[i]))\n",
    "                \n",
    "            elif i == len(self.hidden_dims):\n",
    "                # do stuff for last layer\n",
    "                self.weights[i] = np.random.randn(self.hidden_dims[i-1], self.output_dim)# / np.sqrt(self.hidden_dims[i-1])\n",
    "                self.biases[i] = np.zeros((1, self.output_dim))\n",
    "                \n",
    "            else:\n",
    "                # do stuff for every other middle juicy deep layer\n",
    "                self.weights[i] = np.random.randn(self.hidden_dims[i-1], self.hidden_dims[i])# / np.sqrt(self.hidden_dims[i-1])\n",
    "                self.biases[i] = np.zeros((1, self.hidden_dims[i]))\n",
    "            \n",
    "        \n",
    "        # This is what we return at the end\n",
    "        self.model = {}\n",
    "    \n",
    "    # print some of the network matrices for observation and debugging.\n",
    "    def print_network(self, weights = False, biases = False):\n",
    "        \n",
    "        if weights == True:\n",
    "            for i in range(len(self.weights)):\n",
    "                # print all the weights and their shapes to see if they match up\n",
    "                print(\"weights\", i, \"of shape\", self.weights[i].shape)\n",
    "                print(self.weights[i])\n",
    "        if biases == True: \n",
    "            for i in range(len(self.biases)):\n",
    "                # print all the weights and their shapes to see if they match up\n",
    "                print(\"biases\", i, \"of shape\", self.biases[i].shape)\n",
    "                print(self.biases[i])\n",
    "            \n",
    "            \n",
    "    # calculate the accuracy of your training data\n",
    "    # :param predictions: your actual predictions (1d array)\n",
    "    # :param labels: real labels (1d array)\n",
    "    def calculate_accuracy(self, predictions, labels):\n",
    "        \n",
    "        accuracy = 0\n",
    "        for i in range(len(predictions)):\n",
    "            \n",
    "            if predictions[i] == labels[i]:\n",
    "                accuracy += 1\n",
    "                \n",
    "        return accuracy / len(predictions)\n",
    "    \n",
    "    # feed forward function since it is being called a lot\n",
    "    def feedforward(self, X):\n",
    "        \n",
    "        # Forward propagation to calculate our predictions\n",
    "        probs = []\n",
    "        \n",
    "        for i in range(len(self.hidden_dims) + 1):\n",
    "            if i == 0:\n",
    "                # do stuff for first layer\n",
    "                self.z[i] = X.dot(self.weights[i]) + self.biases[i]\n",
    "                self.a[i] = np.tanh(self.z[i])\n",
    "                \n",
    "            elif i == len(self.hidden_dims):\n",
    "                # do stuff for last layer\n",
    "                self.z[i] = self.a[i-1].dot(self.weights[i]) + self.biases[i]\n",
    "                # calculate the probabilities of the output layer\n",
    "                probs = softmax(self.z[i])\n",
    "                \n",
    "            else:\n",
    "                # do stuff for every other middle juicy deep layer\n",
    "                self.z[i] = self.a[i-1].dot(self.weights[i]) + self.biases[i]\n",
    "                self.a[i] = np.tanh(self.z[i])\n",
    "                \n",
    "        return probs\n",
    "    \n",
    "    # Helpers function to evaluate the total loss on the dataset\n",
    "    def calculate_loss(self, model):\n",
    "        \n",
    "        # feed forward to get our predictions\n",
    "        probs = self.feedforward(self.X)            \n",
    "        \n",
    "        correct_logprobs = -np.log(probs[range(self.samples), np.matrix(self.y).A1.astype(int)])\n",
    "        data_loss = np.sum(correct_logprobs)\n",
    "        \n",
    "        # Add regulatization term to loss (optional)\n",
    "        if self.regularization == True:\n",
    "            sum = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                # add the sum to a cumulative variable for use later to calculate data loss\n",
    "                sum += np.sum(np.square(self.weights[i]))\n",
    "\n",
    "            data_loss += self.reg_lambda/2 * sum\n",
    "        \n",
    "        return 1./self.samples*data_loss\n",
    "    \n",
    "    # Helper function to predict an output (0 or 1)\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # feed forward to get our predictions\n",
    "        probs = self.feedforward(X)\n",
    "        \n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    # return the model with the weights and biases\n",
    "    def return_model(self):\n",
    "        return { 'weights': self.weights, 'biases': self.biases }\n",
    "    \n",
    "    # This function learns parameters for the neural network and returns the model.\n",
    "    def build_model(self, \n",
    "                    num_passes=20000, \n",
    "                    batch_size=100, \n",
    "                    print_loss=True, \n",
    "                    print_loss_iteration=1000,\n",
    "                    print_delta_shapes=False, \n",
    "                    print_deltas=False,\n",
    "                    regularization=True):\n",
    "        \n",
    "        # loop through the batches\n",
    "        for k in range(math.ceil(self.samples/batch_size)):\n",
    "            print(\"batch\",k)\n",
    "            batch_X = self.X[(k*batch_size):((k*batch_size)+batch_size), :]\n",
    "            batch_y = self.y[(k*batch_size):((k*batch_size)+batch_size), :]\n",
    "            \n",
    "            self.learning_rate = self.learning_rate/2\n",
    "            \n",
    "#             # reinitialize deltas\n",
    "#             # preactivation layer arrays\n",
    "#             self.z = [[None] for i in range(len(self.hidden_dims)+1)] \n",
    "#             # activation layer arrays\n",
    "#             self.a = [[None] for i in range(len(self.hidden_dims)+1)]\n",
    "#             # deltas arrays\n",
    "#             self.deltas = [[None] for i in range(len(self.hidden_dims)+1)]\n",
    "#             self.deltaWeights = [[None] for i in range(len(self.hidden_dims)+1)]\n",
    "#             self.deltaBiases = [[None] for i in range(len(self.hidden_dims)+1)]\n",
    "            \n",
    "            # Gradient descent. For each batch...\n",
    "            for j in range(0, num_passes):\n",
    "\n",
    "                # print deltas and shapes if required\n",
    "#                 print_delta_shapes = True\n",
    "                if j == 1:\n",
    "                    if print_delta_shapes == True:\n",
    "                        print(\"deltas shape\",np.array(self.deltas[1]).shape)\n",
    "                        print(\"delta weights shape\",np.array(self.deltaWeights).shape)\n",
    "                        print(\"delta biases shape\",np.matrix(self.deltaBiases).shape)\n",
    "                    if print_deltas == True:\n",
    "                        print(\"deltas\",self.deltas)\n",
    "                        print(\"delta weights\",self.deltaWeights)\n",
    "                        print(\"delta biases\",self.deltaBiases)\n",
    "\n",
    "                # feed forward to get our predictions\n",
    "                probs = self.feedforward(batch_X)\n",
    "\n",
    "                # BACKPROPAGATION\n",
    "\n",
    "                # let's go in the opposite direction this time ;)\n",
    "                for i in reversed(range(len(self.hidden_dims) + 1)):\n",
    "                    if i == len(self.hidden_dims):\n",
    "                        # do stuff for first delta (which is actually the last in the array)\n",
    "                        self.deltas[i] = probs\n",
    "                        \n",
    "                        self.deltas[i][range(len(batch_y)), np.matrix(batch_y).A1.astype(int)] -= 1 \n",
    "                        self.deltaWeights[i] = (self.a[i-1].T).dot(self.deltas[i])\n",
    "                        self.deltaBiases[i] = np.sum(self.deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "                    elif i == 0:\n",
    "                        # do stuff for first layer\n",
    "                        self.deltas[i] = self.deltas[i+1].dot(self.weights[i+1].T) * (1 - np.power(self.a[i], 2))\n",
    "                        self.deltaWeights[i] = np.dot(batch_X.T, self.deltas[i])\n",
    "                        self.deltaBiases[i] = np.sum(self.deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "    #                 elif i == 0:\n",
    "    #                     # do nothing\n",
    "    #                     print(\"end of backprop loop\",i)\n",
    "\n",
    "                    elif i != 0:\n",
    "                        # do stuff for every other middle juicy deep layer\n",
    "                        self.deltas[i] = self.deltas[i+1].dot(self.weights[i+1].T) * (1 - np.power(self.a[i], 2))\n",
    "                        self.deltaWeights[i] = (self.a[i-1].T).dot(self.deltas[i])\n",
    "                        self.deltaBiases[i] = np.sum(self.deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "                if self.regularization == True:\n",
    "                    for i in range(len(self.deltaWeights)):\n",
    "                        self.deltaWeights[i] += self.reg_lambda * self.weights[i]\n",
    "\n",
    "                for i in range(len(self.weights)):\n",
    "                    self.weights[i] += -self.learning_rate * self.deltaWeights[i]\n",
    "                    self.biases[i] += -self.learning_rate * self.deltaBiases[i]\n",
    "\n",
    "                # Assign new parameters to the model\n",
    "                model = { 'weights': self.weights, 'biases': self.biases}\n",
    "\n",
    "                # Optionally print the loss.\n",
    "                # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "                if print_loss and j % print_loss_iteration == 0:\n",
    "                    print (\"Loss after iteration %i: %f\" %(j, self.calculate_loss(model)))\n",
    "            \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [], [], []]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listy = [[] for i in range(3+2)]\n",
    "listy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(3+2)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90918896 0.60759251 0.27096357 0.30120779 0.56633734]\n",
      " [0.90595341 0.63143095 0.         0.21764851 0.58596995]\n",
      " [0.85591027 0.48827251 0.         0.21543012 0.42108007]\n",
      " [0.91846419 0.53590678 0.29295107 0.38862871 0.50583808]\n",
      " [0.8580673  0.48290407 0.         0.21641607 0.41772893]]\n",
      "[[0.909 0.608 0.271 0.301 0.566]\n",
      " [0.906 0.631 0.    0.218 0.586]\n",
      " [0.856 0.488 0.    0.215 0.421]\n",
      " [0.918 0.536 0.293 0.389 0.506]\n",
      " [0.858 0.483 0.    0.216 0.418]]\n",
      "X train (5700, 5)\n",
      "y train (5700, 1)\n",
      "X test (2443, 5)\n",
      "y test (2443, 1)\n"
     ]
    }
   ],
   "source": [
    "occupied_data = pd.read_csv(\"./data/occupied.csv\")\n",
    "formatted_data = np.array(occupied_data)\n",
    "\n",
    "np.random.shuffle(formatted_data)\n",
    "y = np.array(formatted_data[:, [-1]])\n",
    "X = normalize(formatted_data[:, :-1])\n",
    "\n",
    "test_train_split = 0.7\n",
    "y_train = y[:int(np.rint(y.shape[0]*test_train_split))]\n",
    "X_train = X[:int(np.rint(X.shape[0]*test_train_split))]\n",
    "y_test = y[int(np.rint(y.shape[0]*test_train_split)):]\n",
    "X_test = X[int(np.rint(X.shape[0]*test_train_split)):]\n",
    "\n",
    "# show a quick peak of what the data looks like\n",
    "print(X[:5])\n",
    "\n",
    "print(np.around(X, decimals=3)[:5])\n",
    "# X = np.around(X, decimals=3)\n",
    "print(\"X train\",X_train.shape)\n",
    "print(\"y train\",y_train.shape)\n",
    "print(\"X test\",X_test.shape)\n",
    "print(\"y test\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the neural network with our data\n",
    "nn = MLP_NeuralNetwork(X_train, \n",
    "                       y_train, \n",
    "                       X_train.shape[1], \n",
    "                       [6, 20, 10], \n",
    "                       len(np.unique(y)), \n",
    "                       learning_rate=0.01,\n",
    "                       regularization_rate=0.1,\n",
    "                      regularization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights 0 of shape (5, 6)\n",
      "[[-0.75061472  1.31635732  1.24614003 -1.60491574 -1.46814368 -1.71507046]\n",
      " [ 1.85878369  0.08758798 -0.0523222   0.55547164 -0.96340369 -0.18032147]\n",
      " [-1.18340916  0.60544592 -0.95165055  0.36085606  1.06061026 -0.11715218]\n",
      " [ 0.82566485 -1.20981355 -1.19050362  0.21987182 -0.21291913 -1.41149914]\n",
      " [-0.48298102  1.20176208 -0.70580303  0.02518135 -0.39112815  0.19313912]]\n",
      "weights 1 of shape (6, 20)\n",
      "[[-1.66928352  0.56321749  0.42038343 -1.60135911  0.6323367  -1.19206718\n",
      "  -1.23504906  1.77230263 -1.12232183 -2.19466822 -0.51247281 -0.37289948\n",
      "  -0.06576479 -0.96246589 -0.34202888 -0.61730982 -0.49024521  0.89178569\n",
      "   0.12880684 -0.07176304]\n",
      " [-0.56470676  1.05636531 -1.534368   -0.04788228  1.95008737 -0.88800014\n",
      "  -0.06925655  0.52853321  1.99710421 -0.86696281  0.11467227  0.84500322\n",
      "  -2.40551234  1.50294697  0.47081329  0.14421742 -2.19915021 -0.92906958\n",
      "   1.56258821  1.04432302]\n",
      " [-0.76777642  1.06675011 -0.93058745 -0.63948682  0.99225092 -1.07060122\n",
      "  -0.22403674  0.59597991  0.9703307   1.38472774 -0.49420795 -1.71325333\n",
      "   0.07387356  0.76055098 -0.22118064  0.82234344  0.88291546  0.20923182\n",
      "   0.39680107 -0.43361653]\n",
      " [ 0.48519688 -1.13806123  1.76556172  0.10892319  0.71143484 -1.10179758\n",
      "  -1.3227559  -0.78774716  1.41202232 -0.58877917  0.74877037 -0.20075826\n",
      "  -0.15771959  1.78656962  0.67112351 -1.16814599  1.76885914  1.60336862\n",
      "   0.41117396 -0.4659153 ]\n",
      " [ 0.98866488  0.32714112  0.14131411 -1.13348885  0.63496231 -2.15926477\n",
      "   0.88133076 -0.75166035 -0.71960043 -0.81828044 -1.55352082  0.31907125\n",
      "  -1.5039176   0.25071705  1.11781648  0.59060095  1.92715024  0.38656316\n",
      "   0.71557649 -0.08347004]\n",
      " [-0.80795299  0.30835973  0.22059011  1.33309394  0.20739424  0.60444543\n",
      "  -0.51651791  0.13374375 -1.22697075 -2.13941724 -1.2303917   0.250618\n",
      "   1.63748733 -0.03472678 -0.91093945  0.7600424   2.54289735  0.53995104\n",
      "   0.86621646 -1.89792431]]\n",
      "weights 2 of shape (20, 10)\n",
      "[[-0.29050317  0.11212805  1.25079512 -1.36088997  0.09993288 -0.0479911\n",
      "  -0.35622969 -1.08855909 -0.3510177   2.58839995]\n",
      " [-0.50565079 -0.58430921 -1.27309938  0.74678304 -0.55834612 -0.11080156\n",
      "  -1.18733663 -1.00822592 -0.68986173 -0.08564259]\n",
      " [ 0.82275325  0.9160238   1.71684811  0.51926009  1.82871606  1.08241229\n",
      "   0.36862427  0.29450662 -1.3362037  -1.0434206 ]\n",
      " [-1.63699397  1.32806443  0.41748389  0.71774469  1.00215744 -0.20452716\n",
      "  -0.03291977 -0.3731369   0.31578775 -0.30474632]\n",
      " [ 0.36265652 -0.56065062 -0.30137358  0.32172321  0.36036677 -0.19546904\n",
      "  -1.34061255  1.09461724 -1.17563726 -0.29764721]\n",
      " [-0.5536131  -0.55504099  0.44622177  0.83254686 -0.30073121 -0.69042669\n",
      "  -0.20401458  0.03368243  0.70058937 -0.99632444]\n",
      " [-0.78191269 -1.43285465 -0.14423926 -1.29277998  0.55552091  1.11099307\n",
      "   0.25601231  0.7545576  -1.94387226  0.37071929]\n",
      " [ 0.32930397 -0.73396074 -3.67921123  0.08685063  0.37767371  2.49294241\n",
      "   0.25296193 -0.18828602 -1.10957457  0.10389845]\n",
      " [ 1.33236628  1.10697804 -0.39600141 -1.24028162  0.65592332  2.3800806\n",
      "  -0.97465602  0.66019641  0.96768161 -0.14074609]\n",
      " [-0.39963075  0.3323779   0.36678357  0.67226995 -0.12611542 -0.31621674\n",
      "   0.77327249 -1.71830569  1.12988437  0.35091547]\n",
      " [-0.32824567 -1.62521511 -1.02426843  0.37539967 -0.96345417 -0.26032272\n",
      "  -1.97209994 -1.06474405  1.18827442  0.35328473]\n",
      " [ 2.08264754 -1.19294964 -0.75650361 -0.20815748 -0.19142847  0.78477753\n",
      "  -0.74687869 -1.05484016  0.66579137 -2.60118845]\n",
      " [-2.18826743  0.36622672  1.13582741  1.19726434  0.92656206 -1.51546525\n",
      "  -0.10474508 -0.21849695  0.06701449 -0.99541373]\n",
      " [ 0.00927629  0.88460153  0.95252957 -0.02895124  0.00640755  0.64857375\n",
      "   0.25908303  0.69260097 -1.17866246  0.02402892]\n",
      " [-0.48901184 -0.08480528  0.0414084  -0.54933132 -0.26766056  0.99574867\n",
      "   1.95436584 -0.43325078  0.69268447  1.56396518]\n",
      " [ 1.33041748  0.39310899 -1.67787026  1.55770905 -0.42189618  0.4796616\n",
      "  -0.52375925 -0.38214783  0.29984661 -0.85253908]\n",
      " [ 1.41772596 -0.06789456  1.08879181  0.32428309  0.68694395 -0.17776856\n",
      "   0.28300478 -1.07059495  1.44463407  0.52406233]\n",
      " [ 0.96480208 -2.31816762 -1.53401192 -0.31804907  0.98850527 -0.41576608\n",
      "  -0.89778162 -1.10701147 -0.70089353 -0.5894653 ]\n",
      " [ 0.82099799  0.1962925  -1.45971563  0.09465309  1.12398924 -0.77985331\n",
      "  -1.40684865  0.31679264 -0.55351967  1.02787946]\n",
      " [-0.57271403  0.36825637  0.23810333  1.04871929  0.14737567  0.77393101\n",
      "   1.18358495 -1.86111286  1.2897397  -0.33102865]]\n",
      "weights 3 of shape (10, 2)\n",
      "[[-0.31232848  0.33928471]\n",
      " [-0.15590853 -0.50178967]\n",
      " [ 0.23556889 -1.76360526]\n",
      " [-1.09586204 -1.08776574]\n",
      " [-0.30517005 -0.47374837]\n",
      " [-0.20059454  0.35519677]\n",
      " [ 0.68951772  0.41058968]\n",
      " [-0.56497844  0.59939069]\n",
      " [-0.16293631  1.6002145 ]\n",
      " [ 0.6816272   0.0148801 ]]\n"
     ]
    }
   ],
   "source": [
    "nn.print_network(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8129907782335066"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.calculate_loss({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "Loss after iteration 0: 0.652180\n",
      "Loss after iteration 1000: 1.433738\n",
      "Loss after iteration 2000: 0.735521\n",
      "Loss after iteration 3000: 0.520714\n",
      "Loss after iteration 4000: 1.348598\n",
      "batch 1\n",
      "Loss after iteration 0: 0.652261\n",
      "Loss after iteration 1000: 0.509487\n",
      "Loss after iteration 2000: 0.509479\n",
      "Loss after iteration 3000: 0.509422\n",
      "Loss after iteration 4000: 0.059579\n",
      "batch 2\n",
      "Loss after iteration 0: 0.512303\n",
      "Loss after iteration 1000: 0.512447\n",
      "Loss after iteration 2000: 0.512359\n",
      "Loss after iteration 3000: 0.050629\n",
      "Loss after iteration 4000: 0.050340\n",
      "batch 3\n",
      "Loss after iteration 0: 0.056709\n",
      "Loss after iteration 1000: 0.052893\n",
      "Loss after iteration 2000: 0.054112\n",
      "Loss after iteration 3000: 0.055011\n",
      "Loss after iteration 4000: 0.055525\n",
      "batch 4\n",
      "Loss after iteration 0: 0.053898\n",
      "Loss after iteration 1000: 0.064820\n",
      "Loss after iteration 2000: 0.066806\n",
      "Loss after iteration 3000: 0.068384\n",
      "Loss after iteration 4000: 0.069710\n",
      "batch 5\n",
      "Loss after iteration 0: 0.070215\n",
      "Loss after iteration 1000: 0.088035\n",
      "Loss after iteration 2000: 0.090972\n",
      "Loss after iteration 3000: 0.096060\n",
      "Loss after iteration 4000: 0.098738\n",
      "batch 6\n",
      "Loss after iteration 0: 0.098610\n",
      "Loss after iteration 1000: 0.062807\n",
      "Loss after iteration 2000: 0.062795\n",
      "Loss after iteration 3000: 0.063849\n",
      "Loss after iteration 4000: 0.064365\n",
      "batch 7\n",
      "Loss after iteration 0: 0.063933\n",
      "Loss after iteration 1000: 0.060741\n",
      "Loss after iteration 2000: 0.062414\n",
      "Loss after iteration 3000: 0.064459\n",
      "Loss after iteration 4000: 0.066068\n",
      "batch 8\n",
      "Loss after iteration 0: 0.067117\n",
      "Loss after iteration 1000: 0.061696\n",
      "Loss after iteration 2000: 0.060454\n",
      "Loss after iteration 3000: 0.059739\n",
      "Loss after iteration 4000: 0.059203\n",
      "batch 9\n",
      "Loss after iteration 0: 0.058837\n",
      "Loss after iteration 1000: 0.062063\n",
      "Loss after iteration 2000: 0.063407\n",
      "Loss after iteration 3000: 0.065462\n",
      "Loss after iteration 4000: 0.067159\n",
      "batch 10\n",
      "Loss after iteration 0: 0.067803\n",
      "Loss after iteration 1000: 0.056652\n",
      "Loss after iteration 2000: 0.056409\n"
     ]
    }
   ],
   "source": [
    "# Build a model with the given parameters\n",
    "model = nn.build_model(num_passes=5000, \n",
    "                       print_loss=True,\n",
    "                       print_loss_iteration=1000,\n",
    "                       batch_size=200, \n",
    "                       print_deltas=False, \n",
    "                       print_delta_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0\n",
      " 1 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
      " 0 0 1 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 1 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 1 1 0 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0\n",
      " 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 1\n",
      " 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0\n",
      " 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1\n",
      " 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0\n",
      " 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0\n",
      " 0 0 0 0 1 0 1 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0\n",
      " 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0\n",
      " 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1\n",
      " 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0\n",
      " 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = nn.predict(X_test)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "print(np.array(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = y_test.astype(int).squeeze()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9852640196479738"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.calculate_accuracy(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
