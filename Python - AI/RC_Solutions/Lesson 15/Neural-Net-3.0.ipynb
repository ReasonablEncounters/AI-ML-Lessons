{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transfer functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid\n",
    "def dsigmoid(y):\n",
    "    return y * (1.0 - y)\n",
    "\n",
    "# using softmax as output layer is recommended for classification where outputs are mutually exclusive\n",
    "def softmax(w):\n",
    "    exp_scores = np.exp(w)\n",
    "    # axis is 1 to sum across rows\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "# using tanh over logistic sigmoid for the hidden layer is recommended   \n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "    \n",
    "# derivative for tanh sigmoid\n",
    "def dtanh(y):\n",
    "    return 1 - y*y\n",
    "\n",
    "# Generate some labels for out data - this will be the true data\n",
    "def generate_labels(samples, classes):\n",
    "    y = np.array([])\n",
    "\n",
    "    for i in range(classes):\n",
    "        y = np.append(y, [i]*samples)\n",
    "\n",
    "    np.random.shuffle(y)\n",
    "    return np.vstack(y[:samples]).astype(int)\n",
    "\n",
    "# Column normalize with negatives\n",
    "def normalize_with_negatives(x):\n",
    "    x = np.array(x)\n",
    "    # ptp(0) returns the \"peak-to-peak\" (i.e. the range, max - min) along axis 0.\n",
    "    # this is necessary to handle negative values and guaranteed the minimum value in each column is 0\n",
    "    x_normed = (x - x.min(0)) / x.ptp(0)\n",
    "    return x_normed\n",
    "\n",
    "def normalize(x):\n",
    "    x = np.array(x)\n",
    "    x_normed = x / x.max(axis=0)\n",
    "    return x_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP_NeuralNetwork(object):\n",
    "    \n",
    "    # Class constructor\n",
    "    def __init__(self, \n",
    "                 X, \n",
    "                 y, \n",
    "                 input_dim, \n",
    "                 hidden_dims, \n",
    "                 output_dim, \n",
    "                 iterations = 50, \n",
    "                 learning_rate = 0.001,\n",
    "                 regularization = True,\n",
    "                 regularization_rate = 0.01):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        :param X: training samples\n",
    "        :param y: training labels\n",
    "        \n",
    "        :param input_dim: number of input neurons\n",
    "        :param hidden_dims: number of hidden neurons in an array\n",
    "        :param output_dim: number of output neurons\n",
    "        \n",
    "        :param iterations: how many epochs\n",
    "        :param learning_rate: initial learning rate\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize parameters\n",
    "        self.regularization = regularization\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = regularization_rate\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.samples = X.shape[0]\n",
    "        \n",
    "        self.hidden_dims = hidden_dims\n",
    "        \n",
    "        # training input and labels\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        # +1 as we need n layers - 1 weights and biases\n",
    "        self.weights = [[] for i in range(len(self.hidden_dims)+1)]        \n",
    "        self.biases = [[] for i in range(len(self.hidden_dims)+1)]    \n",
    "        \n",
    "        # preactivation layer arrays\n",
    "        self.z = [[] for i in range(len(self.hidden_dims)+1)] \n",
    "        # activation layer arrays\n",
    "        self.a = [[] for i in range(len(self.hidden_dims)+1)]\n",
    "        # deltas arrays\n",
    "        self.deltas = [[] for i in range(len(self.hidden_dims)+1)]\n",
    "        self.deltaWeights = [[] for i in range(len(self.hidden_dims)+1)]\n",
    "        self.deltaBiases = [[] for i in range(len(self.hidden_dims)+1)]\n",
    "\n",
    "        \n",
    "        # Initialize the parameters to random values. We need to learn these.\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        # the reason we are using the size of hidden_dims + 1 is because we need \n",
    "        # weights and biases for the total number of layers - 1\n",
    "        for i in range(len(self.hidden_dims)+1):\n",
    "            np.random.seed(np.random.randint(99))\n",
    "            if i == 0:\n",
    "                # do stuff for first layer\n",
    "                self.weights[i] = np.random.randn(self.input_dim, self.hidden_dims[i])# / np.sqrt(self.input_dim)\n",
    "                self.biases[i] = np.zeros((1, self.hidden_dims[i]))\n",
    "                \n",
    "            elif i == len(self.hidden_dims):\n",
    "                # do stuff for last layer\n",
    "                self.weights[i] = np.random.randn(self.hidden_dims[i-1], self.output_dim)# / np.sqrt(self.hidden_dims[i-1])\n",
    "                self.biases[i] = np.zeros((1, self.output_dim))\n",
    "                \n",
    "            else:\n",
    "                # do stuff for every other middle juicy deep layer\n",
    "                self.weights[i] = np.random.randn(self.hidden_dims[i-1], self.hidden_dims[i])# / np.sqrt(self.hidden_dims[i-1])\n",
    "                self.biases[i] = np.zeros((1, self.hidden_dims[i]))\n",
    "            \n",
    "        \n",
    "        # This is what we return at the end\n",
    "        self.model = {}\n",
    "    \n",
    "    # print some of the network matrices for observation and debugging.\n",
    "    def print_network(self, weights = False, biases = False):\n",
    "        \n",
    "        if weights == True:\n",
    "            for i in range(len(self.weights)):\n",
    "                # print all the weights and their shapes to see if they match up\n",
    "                print(\"weights\", i, \"of shape\", self.weights[i].shape)\n",
    "                print(self.weights[i])\n",
    "        if biases == True: \n",
    "            for i in range(len(self.biases)):\n",
    "                # print all the weights and their shapes to see if they match up\n",
    "                print(\"biases\", i, \"of shape\", self.biases[i].shape)\n",
    "                print(self.biases[i])\n",
    "            \n",
    "            \n",
    "    # calculate the accuracy of your training data\n",
    "    # :param predictions: your actual predictions (1d array)\n",
    "    # :param labels: real labels (1d array)\n",
    "    def calculate_accuracy(self, predictions, labels):\n",
    "        \n",
    "        accuracy = 0\n",
    "        for i in range(len(predictions)):\n",
    "            \n",
    "            if predictions[i] == labels[i]:\n",
    "                accuracy += 1\n",
    "                \n",
    "        return accuracy / len(predictions)\n",
    "    \n",
    "    # feed forward function since it is being called a lot\n",
    "    def feedforward(self, X):\n",
    "        \n",
    "        # Forward propagation to calculate our predictions\n",
    "        probs = []\n",
    "        \n",
    "        for i in range(len(self.hidden_dims) + 1):\n",
    "            if i == 0:\n",
    "                # do stuff for first layer\n",
    "                self.z[i] = X.dot(self.weights[i]) + self.biases[i]\n",
    "                self.a[i] = np.tanh(self.z[i])\n",
    "                \n",
    "            elif i == len(self.hidden_dims):\n",
    "                # do stuff for last layer\n",
    "                self.z[i] = self.a[i-1].dot(self.weights[i]) + self.biases[i]\n",
    "                # calculate the probabilities of the output layer\n",
    "                probs = softmax(self.z[i])\n",
    "                \n",
    "            else:\n",
    "                # do stuff for every other middle juicy deep layer\n",
    "                self.z[i] = self.a[i-1].dot(self.weights[i]) + self.biases[i]\n",
    "                self.a[i] = np.tanh(self.z[i])\n",
    "                \n",
    "        return probs\n",
    "    \n",
    "    # Helpers function to evaluate the total loss on the dataset\n",
    "    def calculate_loss(self, model):\n",
    "        \n",
    "        # feed forward to get our predictions\n",
    "        probs = self.feedforward(self.X)            \n",
    "        \n",
    "        correct_logprobs = -np.log(probs[range(self.samples), np.matrix(self.y).A1.astype(int)])\n",
    "        data_loss = np.sum(correct_logprobs)\n",
    "        \n",
    "        # Add regulatization term to loss (optional)\n",
    "        if self.regularization == True:\n",
    "            sum = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                # add the sum to a cumulative variable for use later to calculate data loss\n",
    "                sum += np.sum(np.square(self.weights[i]))\n",
    "\n",
    "            data_loss += self.reg_lambda/2 * sum\n",
    "        \n",
    "        return 1./self.samples*data_loss\n",
    "    \n",
    "    # Helper function to predict an output (0 or 1)\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # feed forward to get our predictions\n",
    "        probs = self.feedforward(X)\n",
    "        \n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    # return the model with the weights and biases\n",
    "    def return_model(self):\n",
    "        return { 'weights': self.weights, 'biases': self.biases }\n",
    "    \n",
    "    # This function learns parameters for the neural network and returns the model.\n",
    "    def build_model(self, \n",
    "                    num_passes=20000, \n",
    "                    batch_size=100, \n",
    "                    print_loss=True, \n",
    "                    print_loss_iteration=1000,\n",
    "                    print_delta_shapes=False, \n",
    "                    print_deltas=False,\n",
    "                    regularization=True):\n",
    "        \n",
    "        # loop through the batches\n",
    "        for k in range(math.ceil(self.samples/batch_size)):\n",
    "            print(\"batch\",k)\n",
    "            batch_X = self.X[(k*batch_size):((k*batch_size)+batch_size), :]\n",
    "            batch_y = self.y[(k*batch_size):((k*batch_size)+batch_size), :]\n",
    "            \n",
    "            self.learning_rate = self.learning_rate/2\n",
    "            \n",
    "#             # reinitialize deltas\n",
    "#             # preactivation layer arrays\n",
    "#             self.z = [[None] for i in range(len(self.hidden_dims)+1)] \n",
    "#             # activation layer arrays\n",
    "#             self.a = [[None] for i in range(len(self.hidden_dims)+1)]\n",
    "#             # deltas arrays\n",
    "#             self.deltas = [[None] for i in range(len(self.hidden_dims)+1)]\n",
    "#             self.deltaWeights = [[None] for i in range(len(self.hidden_dims)+1)]\n",
    "#             self.deltaBiases = [[None] for i in range(len(self.hidden_dims)+1)]\n",
    "            \n",
    "            # Gradient descent. For each batch...\n",
    "            for j in range(0, num_passes):\n",
    "\n",
    "                # print deltas and shapes if required\n",
    "#                 print_delta_shapes = True\n",
    "                if j == 1:\n",
    "                    if print_delta_shapes == True:\n",
    "                        print(\"deltas shape\",np.array(self.deltas[1]).shape)\n",
    "                        print(\"delta weights shape\",np.array(self.deltaWeights).shape)\n",
    "                        print(\"delta biases shape\",np.matrix(self.deltaBiases).shape)\n",
    "                    if print_deltas == True:\n",
    "                        print(\"deltas\",self.deltas)\n",
    "                        print(\"delta weights\",self.deltaWeights)\n",
    "                        print(\"delta biases\",self.deltaBiases)\n",
    "\n",
    "                # feed forward to get our predictions\n",
    "                probs = self.feedforward(batch_X)\n",
    "\n",
    "                # BACKPROPAGATION\n",
    "\n",
    "                # let's go in the opposite direction this time ;)\n",
    "                for i in reversed(range(len(self.hidden_dims) + 1)):\n",
    "                    if i == len(self.hidden_dims):\n",
    "                        # do stuff for first delta (which is actually the last in the array)\n",
    "                        self.deltas[i] = probs\n",
    "                        \n",
    "                        self.deltas[i][range(len(batch_y)), np.matrix(batch_y).A1.astype(int)] -= 1 \n",
    "                        self.deltaWeights[i] = (self.a[i-1].T).dot(self.deltas[i])\n",
    "                        self.deltaBiases[i] = np.sum(self.deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "                    elif i == 0:\n",
    "                        # do stuff for first layer\n",
    "                        self.deltas[i] = self.deltas[i+1].dot(self.weights[i+1].T) * (1 - np.power(self.a[i], 2))\n",
    "                        self.deltaWeights[i] = np.dot(batch_X.T, self.deltas[i])\n",
    "                        self.deltaBiases[i] = np.sum(self.deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "    #                 elif i == 0:\n",
    "    #                     # do nothing\n",
    "    #                     print(\"end of backprop loop\",i)\n",
    "\n",
    "                    elif i != 0:\n",
    "                        # do stuff for every other middle juicy deep layer\n",
    "                        self.deltas[i] = self.deltas[i+1].dot(self.weights[i+1].T) * (1 - np.power(self.a[i], 2))\n",
    "                        self.deltaWeights[i] = (self.a[i-1].T).dot(self.deltas[i])\n",
    "                        self.deltaBiases[i] = np.sum(self.deltas[i], axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "                if self.regularization == True:\n",
    "                    for i in range(len(self.deltaWeights)):\n",
    "                        self.deltaWeights[i] += self.reg_lambda * self.weights[i]\n",
    "\n",
    "                for i in range(len(self.weights)):\n",
    "                    self.weights[i] += -self.learning_rate * self.deltaWeights[i]\n",
    "                    self.biases[i] += -self.learning_rate * self.deltaBiases[i]\n",
    "\n",
    "                # Assign new parameters to the model\n",
    "                model = { 'weights': self.weights, 'biases': self.biases}\n",
    "\n",
    "                # Optionally print the loss.\n",
    "                # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "                if print_loss and j % print_loss_iteration == 0:\n",
    "                    print (\"Loss after iteration %i: %f\" %(j, self.calculate_loss(model)))\n",
    "            \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [], [], []]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listy = [[] for i in range(3+2)]\n",
    "listy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(3+2)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./data/occupied.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-6f42e6efbc95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moccupied_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data/occupied.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mformatted_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moccupied_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:8873)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'./data/occupied.csv' does not exist"
     ]
    }
   ],
   "source": [
    "occupied_data = pd.read_csv(\"./data/occupied.csv\")\n",
    "formatted_data = np.array(occupied_data)\n",
    "\n",
    "np.random.shuffle(formatted_data)\n",
    "y = np.array(formatted_data[:, [-1]])\n",
    "X = normalize(formatted_data[:, :-1])\n",
    "\n",
    "test_train_split = 0.7\n",
    "y_train = y[:int(np.rint(y.shape[0]*test_train_split))]\n",
    "X_train = X[:int(np.rint(X.shape[0]*test_train_split))]\n",
    "y_test = y[int(np.rint(y.shape[0]*test_train_split)):]\n",
    "X_test = X[int(np.rint(X.shape[0]*test_train_split)):]\n",
    "\n",
    "# show a quick peak of what the data looks like\n",
    "print(X[:5])\n",
    "\n",
    "print(np.around(X, decimals=3)[:5])\n",
    "# X = np.around(X, decimals=3)\n",
    "print(\"X train\",X_train.shape)\n",
    "print(\"y train\",y_train.shape)\n",
    "print(\"X test\",X_test.shape)\n",
    "print(\"y test\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the neural network with our data\n",
    "nn = MLP_NeuralNetwork(X_train, \n",
    "                       y_train, \n",
    "                       X_train.shape[1], \n",
    "                       [6, 20, 10], \n",
    "                       len(np.unique(y)), \n",
    "                       learning_rate=0.01,\n",
    "                       regularization_rate=0.1,\n",
    "                      regularization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.print_network(weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.calculate_loss({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build a model with the given parameters\n",
    "model = nn.build_model(num_passes=5000, \n",
    "                       print_loss=True,\n",
    "                       print_loss_iteration=1000,\n",
    "                       batch_size=200, \n",
    "                       print_deltas=False, \n",
    "                       print_delta_shapes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = nn.predict(X_test)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "print(np.array(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = y_test.astype(int).squeeze()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.calculate_accuracy(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
